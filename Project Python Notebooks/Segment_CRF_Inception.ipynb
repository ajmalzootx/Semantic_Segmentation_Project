{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb368fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, re, sys, random, shutil, cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, ZeroPadding2D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37edbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_patch_size = 256\n",
    "minmaxscaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f494581",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('seg_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = []\n",
    "image_extension = 'jpg' #png\n",
    "image_type = 'images' #masks\n",
    "for tile_id in range(1,8):\n",
    "  for image_id in range(1,10):\n",
    "    image = cv2.imread(f'Tile {tile_id}/{image_type}/image_part_00{image_id}.{image_extension}',1)\n",
    "    if image is not None:\n",
    "      size_x = (image.shape[1]//image_patch_size)*image_patch_size\n",
    "      size_y = (image.shape[0]//image_patch_size)*image_patch_size\n",
    "      image = Image.fromarray(image)\n",
    "      image = image.crop((0,0,size_x,size_y))\n",
    "      image = np.array(image)\n",
    "      patched_images = patchify(image,(image_patch_size,image_patch_size,3),step = image_patch_size)\n",
    "      for i in range(patched_images.shape[0]):\n",
    "        for j in range(patched_images.shape[1]):\n",
    "          individual_patched_image = patched_images[i,j,:,:]\n",
    "          individual_patched_image = minmaxscaler.fit_transform(individual_patched_image.reshape(-1,individual_patched_image.shape[-1])).reshape(individual_patched_image.shape)\n",
    "          individual_patched_image = individual_patched_image[0]\n",
    "          image_dataset.append(individual_patched_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ea8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dataset = []\n",
    "image_extension = 'png' #png\n",
    "image_type = 'masks' #masks\n",
    "for tile_id in range(1,8):\n",
    "  for image_id in range(1,10):\n",
    "    image = cv2.imread(f'Tile {tile_id}/{image_type}/image_part_00{image_id}.{image_extension}',1)\n",
    "    if image is not None:\n",
    "      image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "      size_x = (image.shape[1]//image_patch_size)*image_patch_size\n",
    "      size_y = (image.shape[0]//image_patch_size)*image_patch_size\n",
    "      image = Image.fromarray(image)\n",
    "      image = image.crop((0,0,size_x,size_y))\n",
    "      image = np.array(image)\n",
    "      patched_images = patchify(image,(image_patch_size,image_patch_size,3),step = image_patch_size)\n",
    "      for i in range(patched_images.shape[0]):\n",
    "        for j in range(patched_images.shape[1]):\n",
    "          individual_patched_image = patched_images[i,j,:,:]\n",
    "          individual_patched_image = individual_patched_image[0]\n",
    "          mask_dataset.append(individual_patched_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2da0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mask_dataset)\n",
    "mask_dataset = np.array(mask_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(image_dataset[0])\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_building =  '#3C1098'\n",
    "class_building = class_building.lstrip('#')\n",
    "class_building = np.array(tuple(int(class_building[i:i+2],16) for i in (0,2,4)))\n",
    "\n",
    "class_land =  '#8429F6'\n",
    "class_land = class_land.lstrip('#')\n",
    "class_land= np.array(tuple(int(class_land[i:i+2],16) for i in (0,2,4)))\n",
    "\n",
    "class_road =  '#6EC1E4'\n",
    "class_road = class_road.lstrip('#')\n",
    "class_road = np.array(tuple(int(class_road[i:i+2],16) for i in (0,2,4)))\n",
    "\n",
    "class_vegetation =  '#FEDD3A'\n",
    "class_vegetation = class_vegetation.lstrip('#')\n",
    "class_vegetation = np.array(tuple(int(class_vegetation[i:i+2],16) for i in (0,2,4)))\n",
    "\n",
    "class_water =  '#E2A929'\n",
    "class_water = class_water.lstrip('#')\n",
    "class_water = np.array(tuple(int(class_water[i:i+2],16) for i in (0,2,4)))\n",
    "\n",
    "class_unlabled =  '#9B9B9B'\n",
    "class_unlabled = class_unlabled.lstrip('#')\n",
    "class_unlabled = np.array(tuple(int(class_unlabled[i:i+2],16) for i in (0,2,4)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5198ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_label (label):\n",
    "  label_segment = np.zeros(label.shape, dtype=np.uint8)\n",
    "  label_segment [np.all(label == class_water, axis=-1)] = 0\n",
    "  label_segment [np.all(label == class_land, axis=-1)] = 1\n",
    "  label_segment [np.all(label == class_road, axis=-1)] = 2\n",
    "  label_segment [np.all(label == class_building, axis=-1)] = 3\n",
    "  label_segment [np.all(label == class_vegetation, axis=-1)] = 4\n",
    "  label_segment [np.all(label == class_unlabled, axis=-1)] = 5\n",
    "  label_segment = label_segment[:,:,0]\n",
    "  return label_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba593edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(mask_dataset.shape[0]):\n",
    "  label = rgb_to_label(mask_dataset[i])\n",
    "  labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.expand_dims(labels,axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f53ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_id = np.random.randint(0,len(image_dataset))\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(image_dataset[random_image_id])\n",
    "plt.subplot(122)\n",
    "plt.imshow(labels[random_image_id][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f320f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0][:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5538bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classes = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_training_dataset = image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1945cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_categorical_dataset = to_categorical(labels, num_classes = total_classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c740f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_categorical_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3adebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d34b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(master_training_dataset,labels_categorical_dataset,test_size = 0.15,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf13f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = X_train.shape[2]\n",
    "image_width = X_train.shape[2]\n",
    "image_channels = X_train.shape[3]\n",
    "total_classes = y_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers import concatenate, BatchNormalization, Dropout, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04336487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef(y_true, y_pred):\n",
    "  y_true_flatten = K.flatten(y_true)\n",
    "  y_pred_flatten = K.flatten(y_pred)\n",
    "  intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "  final_coef_value = (intersection + 1.0) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n",
    "  return final_coef_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf348fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_unet_model(n_classes=5, image_height=256, image_width=256, image_channels=1):\n",
    "\n",
    "  inputs = Input((image_height, image_width, image_channels))\n",
    "\n",
    "  source_input = inputs\n",
    "\n",
    "  c1 = Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(source_input)\n",
    "  c1 = Dropout(0.2)(c1)\n",
    "  c1 = Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c1)\n",
    "  p1 = MaxPooling2D((2,2))(c1)\n",
    "\n",
    "  c2 = Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p1)\n",
    "  c2 = Dropout(0.2)(c2)\n",
    "  c2 = Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c2)\n",
    "  p2 = MaxPooling2D((2,2))(c2)\n",
    "\n",
    "  c3 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p2)\n",
    "  c3 = Dropout(0.2)(c3)\n",
    "  c3 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c3)\n",
    "  p3 = MaxPooling2D((2,2))(c3)\n",
    "\n",
    "  c4 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p3)\n",
    "  c4 = Dropout(0.2)(c4)\n",
    "  c4 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c4)\n",
    "  p4 = MaxPooling2D((2,2))(c4)\n",
    "\n",
    "  c5 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p4)\n",
    "  c5 = Dropout(0.2)(c5)\n",
    "  c5 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c5)\n",
    "\n",
    "  u6 = Conv2DTranspose(128, (2,2), strides=(2,2), padding=\"same\")(c5)\n",
    "  u6 = concatenate([u6, c4])\n",
    "  c6 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u6)\n",
    "  c6 = Dropout(0.2)(c6)\n",
    "  c6 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c6)\n",
    "\n",
    "  u7 = Conv2DTranspose(64, (2,2), strides=(2,2), padding=\"same\")(c6)\n",
    "  u7 = concatenate([u7, c3])\n",
    "  c7 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u7)\n",
    "  c7 = Dropout(0.2)(c7)\n",
    "  c7 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c7)\n",
    "\n",
    "  u8 = Conv2DTranspose(32, (2,2), strides=(2,2), padding=\"same\")(c7)\n",
    "  u8 = concatenate([u8, c2])\n",
    "  c8 = Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u8)\n",
    "  c8 = Dropout(0.2)(c8)\n",
    "  c8 = Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c8)\n",
    "\n",
    "  u9 = Conv2DTranspose(16, (2,2), strides=(2,2), padding=\"same\")(c8)\n",
    "  u9 = concatenate([u9, c1], axis=3)\n",
    "  c9 = Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u9)\n",
    "  c9 = Dropout(0.2)(c9)\n",
    "  c9 = Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c9)\n",
    "\n",
    "  outputs = Conv2D(n_classes, (1,1), activation=\"softmax\")(c9)\n",
    "\n",
    "  model = Model(inputs=[inputs], outputs=[outputs])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\", jaccard_coef]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_learning_model():\n",
    "  return multi_unet_model(n_classes=total_classes,\n",
    "                          image_height=image_height,\n",
    "                          image_width=image_width,\n",
    "                          image_channels=image_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51172540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_deep_learning_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb75e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a105aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = sm.losses.DiceLoss(class_weights = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_loss = sm.losses.CategoricalFocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = dice_loss + (1 * focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=total_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_history = model.fit(X_train, y_train,\n",
    "                          batch_size=16,\n",
    "                          verbose=1,\n",
    "                          epochs=50,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bee995",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_a = model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0074c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history_a.history['loss']\n",
    "val_loss = history_a.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'r', label=\"Validation Loss\")\n",
    "plt.title(\"Training Vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9daf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3416354",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf762f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_argmax = np.argmax(y_pred, axis=3)\n",
    "\n",
    "y_test_argmax = np.argmax(y_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_image_number = random.randint(0, len(X_test))\n",
    "\n",
    "test_image = X_test[test_image_number]\n",
    "ground_truth_image = y_test_argmax[test_image_number]\n",
    "\n",
    "test_image_input = np.expand_dims(test_image, 0)\n",
    "\n",
    "prediction = model.predict(test_image_input)\n",
    "predicted_image = np.argmax(prediction, axis=3)\n",
    "predicted_image = predicted_image[0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b8d88",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.subplot(231)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(test_image)\n",
    "plt.subplot(232)\n",
    "plt.title(\"Original Masked image\")\n",
    "plt.imshow(ground_truth_image)\n",
    "plt.subplot(233)\n",
    "plt.title(\"Predicted Image\")\n",
    "plt.imshow(predicted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"segment_model_munet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681394c",
   "metadata": {},
   "source": [
    "# InceptionResnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c100e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_inception_resnetv2_unet(input_shape):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n",
    "    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n",
    "\n",
    "    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n",
    "    s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)         ## (256 x 256)\n",
    "\n",
    "    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n",
    "    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n",
    "\n",
    "    s4 = encoder.get_layer(\"activation_74\").output      ## (61 x 61)\n",
    "    s4 = ZeroPadding2D(( (2, 1),(2, 1) ))(s4)           ## (64 x 64)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = encoder.get_layer(\"activation_161\").output     ## (30 x 30)\n",
    "    b1 = ZeroPadding2D((1, 1))(b1)                      ## (32 x 32)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n",
    "    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n",
    "    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n",
    "    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n",
    "    \n",
    "    \"\"\" Output \"\"\"\n",
    "    dropout = Dropout(0.3)(d4)\n",
    "    outputs = Conv2D(6, 1, padding=\"same\", activation=\"softmax\")(dropout)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"InceptionResNetV2-UNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n",
    "\n",
    "model_inception = build_inception_resnetv2_unet(input_shape = (256, 256, 3))\n",
    "model_inception.compile(optimizer=Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=[dice_coef, \"accuracy\"])\n",
    "model_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcf89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 **(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.0001, 60)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(\n",
    "    exponential_decay_fn,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = 'InceptionResNetV2-UNet.h5',\n",
    "    save_best_only = True, \n",
    "#     save_weights_only = False,\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'auto', \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    min_delta = 0.001, \n",
    "    patience = 12, \n",
    "    mode = 'auto', \n",
    "    verbose = 1,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename= \"model_training.csv\",\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6626cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_inception.fit(X_train, y_train,\n",
    "                          batch_size=16,\n",
    "                          verbose=1,\n",
    "                          epochs=50,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install crfrnn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d27263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from crfrnn_layer import CrfRnnLayer\n",
    "\n",
    "def add_crf_layer(original_model):\n",
    "    original_model.trainable = False\n",
    "\n",
    "    crf_layer = CrfRnnLayer(image_dims=(224, 224),\n",
    "                            num_classes=2,\n",
    "                            theta_alpha=3.,\n",
    "                            theta_beta=160.,\n",
    "                            theta_gamma=3.,\n",
    "                            num_iterations=10,\n",
    "                            name='crfrnn')([original_model.outputs[0], original_model.inputs[0]])\n",
    "\n",
    "    new_crf_model = tf.keras.Model(inputs=original_model.input, outputs=crf_layer)\n",
    "\n",
    "    return new_crf_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_crf_model = add_crf_layer(model_inception)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b27a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 **(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.0001, 60)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(\n",
    "    exponential_decay_fn,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = 'InceptionResNetV2-UNet.h5',\n",
    "    save_best_only = True, \n",
    "#     save_weights_only = False,\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'auto', \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    min_delta = 0.001, \n",
    "    patience = 12, \n",
    "    mode = 'auto', \n",
    "    verbose = 1,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename= \"model_training.csv\",\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_crf = new_crf_model.fit(X_train, y_train,\n",
    "                          batch_size=16,\n",
    "                          verbose=1,\n",
    "                          epochs=50,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras.crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9545ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.crf import CRF\n",
    "\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_inception_resnetv2_unet(input_shape):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n",
    "    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n",
    "\n",
    "    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n",
    "    s2 = ZeroPadding2D(((1, 0), (1, 0)))(s2)           ## (256 x 256)\n",
    "\n",
    "    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n",
    "    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n",
    "\n",
    "    s4 = encoder.get_layer(\"activation_74\").output     ## (61 x 61)\n",
    "    s4 = ZeroPadding2D(((2, 1), (2, 1)))(s4)           ## (64 x 64)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = encoder.get_layer(\"activation_161\").output    ## (30 x 30)\n",
    "    b1 = ZeroPadding2D((1, 1))(b1)                     ## (32 x 32)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)                    ## (64 x 64)\n",
    "    d2 = decoder_block(d1, s3, 256)                    ## (128 x 128)\n",
    "    d3 = decoder_block(d2, s2, 128)                    ## (256 x 256)\n",
    "    d4 = decoder_block(d3, s1, 64)                     ## (512 x 512)\n",
    "\n",
    "    \"\"\" Output \"\"\"\n",
    "    dropout = Dropout(0.3)(d4)\n",
    "    \n",
    "    # Add CRF layer\n",
    "    crf = CRF(6, sparse_target=True)                   # 6 output classes, use sparse_target for one-hot encoded targets\n",
    "    outputs = crf(dropout)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"InceptionResNetV2-UNet-CRF\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c40bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
